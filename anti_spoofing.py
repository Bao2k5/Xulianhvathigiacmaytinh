# -*- coding: utf-8 -*-
"""
Module Anti-Spoofing - Ph√°t hi·ªán gi·∫£ m·∫°o khu√¥n m·∫∑t
Anti-Spoofing Module for Face Liveness Detection
"""

import cv2
import numpy as np
from scipy import ndimage
import config
import os

class AntiSpoofing:
    """Class ph√°t hi·ªán gi·∫£ m·∫°o khu√¥n m·∫∑t (printed photo, video replay, 3D mask)"""
    
    def __init__(self):
        """Kh·ªüi t·∫°o Anti-Spoofing detector"""
        self.blink_counter = 0
        self.blink_frames = []
        self.prev_ear = 0
        
        # Load Haar Cascade cho ph√°t hi·ªán m·∫Øt
        self.eye_cascade = None
        cascade_candidates = []
        try:
            cascade_candidates.append(cv2.data.haarcascades + 'haarcascade_eye.xml')
        except Exception:
            pass

        # Try to find cascade relative to cv2 package
        try:
            cv2_pkg = os.path.dirname(cv2.__file__)
            cascade_candidates.append(os.path.join(cv2_pkg, 'data', 'haarcascade_eye.xml'))
        except Exception:
            pass

        # Try local project paths
        cascade_candidates.append(os.path.join(os.getcwd(), 'data', 'models', 'haarcascade_eye.xml'))
        cascade_candidates.append(os.path.join(os.path.dirname(__file__), 'data', 'models', 'haarcascade_eye.xml'))

        loaded = False
        for p in cascade_candidates:
            if not p:
                continue
            try:
                if os.path.exists(p):
                    clf = cv2.CascadeClassifier(p)
                    # Check if loaded
                    if hasattr(clf, 'empty') and not clf.empty():
                        self.eye_cascade = clf
                        print(f"‚úÖ Loaded eye cascade from: {p}")
                        loaded = True
                        break
            except Exception as e:
                # continue to next candidate
                print(f"‚ö†Ô∏è Error loading cascade from {p}: {e}")
                continue

        if not loaded:
            # As a last resort try default constructor (may still fail internally)
            try:
                clf = cv2.CascadeClassifier()
                if hasattr(clf, 'empty') and not clf.empty():
                    self.eye_cascade = clf
                    print("‚úÖ Loaded default CascadeClassifier (no file path)")
                else:
                    self.eye_cascade = None
                    print("‚ö†Ô∏è Eye cascade not available; eye-based anti-spoofing disabled.")
            except Exception:
                self.eye_cascade = None
                print("‚ö†Ô∏è Eye cascade not available; eye-based anti-spoofing disabled.")
    
    def calculate_ear(self, eye_landmarks):
        """
        T√≠nh Eye Aspect Ratio (EAR) ƒë·ªÉ ph√°t hi·ªán nh√°y m·∫Øt
        
        Args:
            eye_landmarks: T·ªça ƒë·ªô c√°c ƒëi·ªÉm landmark c·ªßa m·∫Øt
        
        Returns:
            ear: Eye Aspect Ratio
        """
        # EAR = (||p2-p6|| + ||p3-p5||) / (2 * ||p1-p4||)
        # p1-p6 l√† c√°c ƒëi·ªÉm landmark c·ªßa m·∫Øt
        
        if eye_landmarks is None or len(eye_landmarks) < 6:
            return 0.0
        
        # T√≠nh kho·∫£ng c√°ch vertical
        vertical1 = np.linalg.norm(eye_landmarks[1] - eye_landmarks[5])
        vertical2 = np.linalg.norm(eye_landmarks[2] - eye_landmarks[4])
        
        # T√≠nh kho·∫£ng c√°ch horizontal
        horizontal = np.linalg.norm(eye_landmarks[0] - eye_landmarks[3])
        
        if horizontal == 0:
            return 0.0
        
        ear = (vertical1 + vertical2) / (2.0 * horizontal)
        return ear
    
    def detect_blink(self, landmarks):
        """
        Ph√°t hi·ªán nh√°y m·∫Øt d·ª±a tr√™n EAR
        
        Args:
            landmarks: Face landmarks t·ª´ MTCNN (5 ƒëi·ªÉm)
        
        Returns:
            is_blinking: True n·∫øu ƒëang nh√°y m·∫Øt
            blink_detected: True n·∫øu ph√°t hi·ªán ƒë∆∞·ª£c nh√°y m·∫Øt ho√†n ch·ªânh
        """
        if landmarks is None:
            return False, False
        
        # MTCNN tr·∫£ v·ªÅ 5 landmarks: left_eye, right_eye, nose, mouth_left, mouth_right
        left_eye = landmarks[0]
        right_eye = landmarks[1]
        
        # Gi·∫£ l·∫≠p EAR ƒë∆°n gi·∫£n d·ª±a tr√™n v·ªã tr√≠ m·∫Øt
        # (Trong th·ª±c t·∫ø, c·∫ßn 68 landmarks ƒë·ªÉ t√≠nh EAR ch√≠nh x√°c)
        eye_distance = np.linalg.norm(left_eye - right_eye)
        
        # Gi·∫£ ƒë·ªãnh: n·∫øu kh√¥ng c√≥ landmarks chi ti·∫øt, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p ƒë∆°n gi·∫£n
        # L∆∞u v√†o buffer ƒë·ªÉ ph√°t hi·ªán pattern
        self.blink_frames.append(eye_distance)
        
        if len(self.blink_frames) > 10:
            self.blink_frames.pop(0)
        
        # Ph√°t hi·ªán bi·∫øn ƒë·ªông ƒë·ªôt ng·ªôt (c√≥ th·ªÉ l√† nh√°y m·∫Øt)
        if len(self.blink_frames) >= 5:
            variance = np.var(self.blink_frames)
            if variance > 10:  # Threshold
                return True, True
        
        return False, False
    
    def texture_analysis(self, face_image):
        """
        Ph√¢n t√≠ch texture ƒë·ªÉ ph√°t hi·ªán ·∫£nh in
        ·∫¢nh in th∆∞·ªùng c√≥ texture ƒë·ªìng nh·∫•t h∆°n khu√¥n m·∫∑t th·∫≠t
        
        Args:
            face_image: numpy array (RGB)
        
        Returns:
            score: ƒêi·ªÉm liveness (0-1), c√†ng cao c√†ng gi·ªëng ng∆∞·ªùi th·∫≠t
        """
        # Convert sang grayscale
        gray = cv2.cvtColor(face_image, cv2.COLOR_RGB2GRAY)
        
        # 1. Ph√¢n t√≠ch Local Binary Pattern (LBP)
        lbp = self._compute_lbp(gray)
        lbp_variance = np.var(lbp)
        
        # 2. Ph√¢n t√≠ch frequency domain (FFT)
        fft = np.fft.fft2(gray)
        fft_shift = np.fft.fftshift(fft)
        magnitude = np.abs(fft_shift)
        
        # Khu√¥n m·∫∑t th·∫≠t c√≥ nhi·ªÅu high-frequency components h∆°n
        h, w = magnitude.shape
        center_h, center_w = h // 2, w // 2
        high_freq = magnitude[0:center_h//2, :].sum() + magnitude[center_h+center_h//2:, :].sum()
        total_freq = magnitude.sum()
        high_freq_ratio = high_freq / (total_freq + 1e-6)
        
        # 3. Ph√¢n t√≠ch edge density
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / (gray.shape[0] * gray.shape[1])
        
        # 4. Ph√¢n t√≠ch color distribution
        # Khu√¥n m·∫∑t th·∫≠t c√≥ ph√¢n b·ªë m√†u ph·ª©c t·∫°p h∆°n
        hsv = cv2.cvtColor(face_image, cv2.COLOR_RGB2HSV)
        hist = cv2.calcHist([hsv], [0, 1], None, [50, 60], [0, 180, 0, 256])
        hist_variance = np.var(hist)
        
        # T·ªïng h·ª£p c√°c ch·ªâ s·ªë
        # Normalize v√† weighted sum
        lbp_score = min(lbp_variance / 1000, 1.0) * 0.25
        freq_score = min(high_freq_ratio * 10, 1.0) * 0.25
        edge_score = min(edge_density * 20, 1.0) * 0.25
        color_score = min(hist_variance / 10000, 1.0) * 0.25
        
        total_score = lbp_score + freq_score + edge_score + color_score
        
        return total_score
    
    def _compute_lbp(self, image):
        """T√≠nh Local Binary Pattern"""
        rows, cols = image.shape
        lbp = np.zeros((rows-2, cols-2), dtype=np.uint8)
        
        for i in range(1, rows-1):
            for j in range(1, cols-1):
                center = image[i, j]
                code = 0
                
                # 8 neighbors
                code |= (image[i-1, j-1] > center) << 7
                code |= (image[i-1, j] > center) << 6
                code |= (image[i-1, j+1] > center) << 5
                code |= (image[i, j+1] > center) << 4
                code |= (image[i+1, j+1] > center) << 3
                code |= (image[i+1, j] > center) << 2
                code |= (image[i+1, j-1] > center) << 1
                code |= (image[i, j-1] > center) << 0
                
                lbp[i-1, j-1] = code
        
        return lbp
    
    def motion_analysis(self, current_frame, prev_frame):
        """
        Ph√¢n t√≠ch chuy·ªÉn ƒë·ªông ƒë·ªÉ ph√°t hi·ªán video replay
        Video replay th∆∞·ªùng c√≥ chuy·ªÉn ƒë·ªông kh√¥ng t·ª± nhi√™n
        
        Args:
            current_frame: Frame hi·ªán t·∫°i (RGB)
            prev_frame: Frame tr∆∞·ªõc ƒë√≥ (RGB)
        
        Returns:
            score: ƒêi·ªÉm liveness (0-1)
        """
        if prev_frame is None:
            return 0.5
        
        # Check if sizes match
        if current_frame.shape != prev_frame.shape:
            return 0.5
        
        # Convert sang grayscale
        gray1 = cv2.cvtColor(prev_frame, cv2.COLOR_RGB2GRAY)
        gray2 = cv2.cvtColor(current_frame, cv2.COLOR_RGB2GRAY)
        
        # Resize to same size if needed
        if gray1.shape != gray2.shape:
            gray2 = cv2.resize(gray2, (gray1.shape[1], gray1.shape[0]))
        
        # T√≠nh optical flow
        flow = cv2.calcOpticalFlowFarneback(
            gray1, gray2, None,
            pyr_scale=0.5, levels=3, winsize=15,
            iterations=3, poly_n=5, poly_sigma=1.2, flags=0
        )
        
        # T√≠nh magnitude v√† angle
        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        
        # Video replay th∆∞·ªùng c√≥ pattern chuy·ªÉn ƒë·ªông ƒë·ªìng nh·∫•t
        # Ng∆∞·ªùi th·∫≠t c√≥ vi chuy·ªÉn ƒë·ªông (micro-movements) t·ª± nhi√™n h∆°n
        
        # 1. T√≠nh variance c·ªßa magnitude
        mag_variance = np.var(magnitude)
        
        # 2. T√≠nh entropy c·ªßa angle distribution
        angle_hist, _ = np.histogram(angle, bins=36, range=(0, 2*np.pi))
        angle_hist = angle_hist / (angle_hist.sum() + 1e-6)
        entropy = -np.sum(angle_hist * np.log2(angle_hist + 1e-6))
        
        # 3. T√≠nh average magnitude
        avg_magnitude = np.mean(magnitude)
        
        # Scoring
        variance_score = min(mag_variance / 10, 1.0) * 0.4
        entropy_score = min(entropy / 5, 1.0) * 0.3
        magnitude_score = min(avg_magnitude / 5, 1.0) * 0.3
        
        total_score = variance_score + entropy_score + magnitude_score
        
        return total_score
    
    def depth_analysis(self, face_image):
        """
        Ph√¢n t√≠ch ƒë·ªô s√¢u ƒë·ªÉ ph√°t hi·ªán m·∫∑t n·∫° 3D ho·∫∑c ·∫£nh ph·∫≥ng
        
        Args:
            face_image: numpy array (RGB)
        
        Returns:
            score: ƒêi·ªÉm liveness (0-1)
        """
        # Convert sang grayscale
        gray = cv2.cvtColor(face_image, cv2.COLOR_RGB2GRAY)
        
        # 1. Ph√¢n t√≠ch gradient ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng ƒë·ªô s√¢u
        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)
        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)
        
        gradient_magnitude = np.sqrt(sobelx**2 + sobely**2)
        
        # 2. Ph√¢n t√≠ch shadow v√† highlight
        # Khu√¥n m·∫∑t 3D th·∫≠t c√≥ shadow/highlight t·ª± nhi√™n
        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # T√≠nh ratio c·ªßa dark v√† bright regions
        dark_ratio = np.sum(binary == 0) / binary.size
        bright_ratio = np.sum(binary == 255) / binary.size
        
        # 3. Ph√¢n t√≠ch contour complexity
        contours, _ = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        contour_complexity = len(contours)
        
        # Scoring
        gradient_score = min(np.mean(gradient_magnitude) / 50, 1.0) * 0.4
        contrast_score = min(abs(dark_ratio - bright_ratio) * 5, 1.0) * 0.3
        complexity_score = min(contour_complexity / 100, 1.0) * 0.3
        
        total_score = gradient_score + contrast_score + complexity_score
        
        return total_score
    
    def check_liveness(self, face_image, landmarks=None, prev_frame=None):
        """
        Ki·ªÉm tra t·ªïng h·ª£p liveness
        
        Args:
            face_image: numpy array (RGB)
            landmarks: Face landmarks t·ª´ MTCNN
            prev_frame: Frame tr∆∞·ªõc ƒë√≥ ƒë·ªÉ ph√¢n t√≠ch motion
        
        Returns:
            is_real: True n·∫øu l√† ng∆∞·ªùi th·∫≠t
            score: ƒêi·ªÉm liveness (0-1)
            details: Dictionary ch·ª©a chi ti·∫øt c√°c ph∆∞∆°ng ph√°p
        """
        scores = {}
        
        # 1. Texture analysis
        if config.USE_TEXTURE_ANALYSIS:
            texture_score = self.texture_analysis(face_image)
            scores['texture'] = texture_score
        
        # 2. Blink detection
        if config.USE_BLINK_DETECTION and landmarks is not None:
            is_blinking, blink_detected = self.detect_blink(landmarks)
            scores['blink'] = 1.0 if blink_detected else 0.3
        
        # 3. Motion analysis
        if config.USE_MOTION_ANALYSIS and prev_frame is not None:
            motion_score = self.motion_analysis(face_image, prev_frame)
            scores['motion'] = motion_score
        
        # 4. Depth analysis
        if config.USE_DEPTH_ANALYSIS:
            depth_score = self.depth_analysis(face_image)
            scores['depth'] = depth_score
        
        # T√≠nh t·ªïng ƒëi·ªÉm (weighted average)
        if len(scores) > 0:
            total_score = np.mean(list(scores.values()))
        else:
            total_score = 0.5
        
        # Quy·∫øt ƒë·ªãnh
        is_real = total_score >= config.LIVENESS_THRESHOLD
        
        return is_real, total_score, scores


# Test module
if __name__ == "__main__":
    print("üß™ Testing Anti-Spoofing Module...")
    
    detector = AntiSpoofing()
    
    # Test v·ªõi webcam
    cap = cv2.VideoCapture(config.CAMERA_INDEX)
    prev_frame = None
    
    print("üìπ Press 'q' to quit")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # Test anti-spoofing
        is_real, score, details = detector.check_liveness(frame, prev_frame=prev_frame)
        
        # Display results
        color = (0, 255, 0) if is_real else (0, 0, 255)
        status = "REAL" if is_real else "FAKE"
        
        cv2.putText(frame, f"Status: {status}", (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
        cv2.putText(frame, f"Score: {score:.2f}", (10, 70),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
        
        # Display detail scores
        y_pos = 110
        for method, method_score in details.items():
            cv2.putText(frame, f"{method}: {method_score:.2f}", (10, y_pos),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            y_pos += 30
        
        cv2.imshow('Anti-Spoofing Test', frame)
        
        prev_frame = frame.copy()
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
    print("‚úÖ Test completed!")
