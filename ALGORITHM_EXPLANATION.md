# ğŸ”¬ GIáº¢I THÃCH THUáº¬T TOÃN CHI TIáº¾T

## Algorithm Explanation for Academic Paper

---

## ğŸ“š Má»¤C Lá»¤C

1. [MTCNN - Face Detection](#1-mtcnn---face-detection)
2. [FaceNet - Face Recognition](#2-facenet---face-recognition)
3. [Anti-Spoofing Methods](#3-anti-spoofing-methods)
4. [Pipeline tá»•ng thá»ƒ](#4-pipeline-tá»•ng-thá»ƒ)
5. [ToÃ¡n há»c Ä‘áº±ng sau](#5-toÃ¡n-há»c-Ä‘áº±ng-sau)

---

## 1. MTCNN - Face Detection

### 1.1 Tá»•ng quan

**MTCNN** (Multi-task Cascaded Convolutional Networks) lÃ  kiáº¿n trÃºc CNN cascade 3 stages Ä‘á»ƒ detect faces vÃ  facial landmarks.

### 1.2 Kiáº¿n trÃºc

```
Input Image (H Ã— W Ã— 3)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 1: Proposal Network (P-Net)      â”‚
â”‚  - Input: Image Pyramid (multiple scales)â”‚
â”‚  - Output: Candidate windows             â”‚
â”‚  - Purpose: Nhanh chÃ³ng tÃ¬m vÃ¹ng cÃ³ thá»ƒ chá»©a faceâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
        Non-Maximum Suppression (NMS)
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 2: Refine Network (R-Net)        â”‚
â”‚  - Input: Candidates tá»« P-Net           â”‚
â”‚  - Output: Refined windows               â”‚
â”‚  - Purpose: Loáº¡i bá» false positives     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
        Non-Maximum Suppression (NMS)
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 3: Output Network (O-Net)        â”‚
â”‚  - Input: Refined candidates            â”‚
â”‚  - Output: Final bounding boxes +       â”‚
â”‚            5 facial landmarks            â”‚
â”‚  - Purpose: XÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c vá»‹ trÃ­   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
    Bounding Boxes + Landmarks
```

### 1.3 Chi tiáº¿t tá»«ng stage

#### Stage 1: P-Net (Proposal Network)

**Input**: Image pyramid vá»›i scales khÃ¡c nhau
**Output**: Candidate face regions

**Network Architecture**:

```
Input (12 Ã— 12 Ã— 3)
    â†“
Conv1: 3Ã—3, stride=1 â†’ (10 Ã— 10 Ã— 10)
    â†“
PReLU + MaxPool 2Ã—2 â†’ (5 Ã— 5 Ã— 10)
    â†“
Conv2: 3Ã—3 â†’ (3 Ã— 3 Ã— 16)
    â†“
PReLU
    â†“
Conv3: 3Ã—3 â†’ (1 Ã— 1 Ã— 32)
    â†“
PReLU
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Face   â”‚ BBox   â”‚        â”‚
â”‚ Cls    â”‚ Reg    â”‚        â”‚
â”‚ (2)    â”‚ (4)    â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Loss Function**:

```
L = L_cls + Î»â‚Â·L_box

Trong Ä‘Ã³:
- L_cls: Binary cross-entropy cho classification
- L_box: Smooth L1 loss cho bounding box regression
- Î»â‚: Trá»ng sá»‘ (thÆ°á»ng = 0.5)
```

#### Stage 2: R-Net (Refine Network)

**Input**: 24 Ã— 24 patches tá»« P-Net
**Output**: Refined face regions

**Network Architecture**:

```
Input (24 Ã— 24 Ã— 3)
    â†“
Conv1: 3Ã—3 â†’ (22 Ã— 22 Ã— 28)
PReLU + MaxPool 3Ã—3 â†’ (11 Ã— 11 Ã— 28)
    â†“
Conv2: 3Ã—3 â†’ (9 Ã— 9 Ã— 48)
PReLU + MaxPool 3Ã—3 â†’ (4 Ã— 4 Ã— 48)
    â†“
Conv3: 2Ã—2 â†’ (3 Ã— 3 Ã— 64)
PReLU
    â†“
FC: 128 units
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Face   â”‚ BBox   â”‚
â”‚ Cls    â”‚ Reg    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Stage 3: O-Net (Output Network)

**Input**: 48 Ã— 48 patches tá»« R-Net
**Output**: Final bounding boxes + 5 landmarks

**Network Architecture**:

```
Input (48 Ã— 48 Ã— 3)
    â†“
Conv1: 3Ã—3 â†’ (46 Ã— 46 Ã— 32)
PReLU + MaxPool 3Ã—3 â†’ (23 Ã— 23 Ã— 32)
    â†“
Conv2: 3Ã—3 â†’ (21 Ã— 21 Ã— 64)
PReLU + MaxPool 3Ã—3 â†’ (10 Ã— 10 Ã— 64)
    â†“
Conv3: 3Ã—3 â†’ (8 Ã— 8 Ã— 64)
PReLU + MaxPool 2Ã—2 â†’ (4 Ã— 4 Ã— 64)
    â†“
Conv4: 2Ã—2 â†’ (3 Ã— 3 Ã— 128)
PReLU
    â†“
FC: 256 units
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Face   â”‚ BBox   â”‚ Landmark â”‚
â”‚ Cls    â”‚ Reg    â”‚ (10)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**5 Landmarks**:

1. Left eye center
2. Right eye center
3. Nose tip
4. Left mouth corner
5. Right mouth corner

### 1.4 Training Details

**Multi-task Loss**:

```
L = Î±Â·L_cls + Î²Â·L_box + Î³Â·L_landmark

Trong Ä‘Ã³:
- L_cls: Face classification loss
- L_box: Bounding box regression loss
- L_landmark: Facial landmark localization loss
- Î±, Î², Î³: Trá»ng sá»‘ (thÆ°á»ng 1, 0.5, 0.5)
```

**Optimization**:

- Optimizer: Stochastic Gradient Descent (SGD)
- Learning rate: 0.001 (vá»›i decay)
- Batch size: 384
- Data augmentation: Flip, rotate, scale

---

## 2. FaceNet - Face Recognition

### 2.1 Tá»•ng quan

**FaceNet** há»c má»™t embedding space trong Ä‘Ã³ khoáº£ng cÃ¡ch Euclidean tÆ°Æ¡ng á»©ng vá»›i similarity giá»¯a cÃ¡c khuÃ´n máº·t.

**Má»¥c tiÃªu**:

```
Same person â†’ Small distance
Different people â†’ Large distance
```

### 2.2 Architecture: InceptionResnetV1

```
Input Face Image (160 Ã— 160 Ã— 3)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stem (Initial Conv layers)       â”‚
â”‚   Conv 3Ã—3, stride=2               â”‚
â”‚   Conv 3Ã—3                         â”‚
â”‚   Conv 3Ã—3, stride=2               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Inception-Resnet-A Ã— 5           â”‚
â”‚   (Mixed layers with residual)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    Reduction-A
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Inception-Resnet-B Ã— 10          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    Reduction-B
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Inception-Resnet-C Ã— 5           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    Global Average Pooling
                 â†“
    Dropout (keep_prob=0.8)
                 â†“
    Fully Connected (512 units)
                 â†“
    L2 Normalization
                 â†“
    512-D Embedding Vector
```

### 2.3 Inception-Resnet Block

**Inception-Resnet-A**:

```
Input
  â†“
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚1Ã—1â”‚1Ã—1â”‚1Ã—1â”‚
â”‚   â”‚â†’5Ã—5â”‚â†’3Ã—3â”‚
â”‚   â”‚   â”‚â†’3Ã—3â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
  â†“   â†“   â†“
  Concat
    â†“
  1Ã—1 Conv (Linear)
    â†“
  Scale (Î»=0.17)
    â†“
    + â† Input (Residual)
    â†“
  ReLU
```

### 2.4 Triplet Loss

**Core Idea**: Há»c embedding sao cho:

- Anchor-Positive gáº§n nhau
- Anchor-Negative xa nhau

**Triplet**:

- **Anchor** (A): áº¢nh gá»‘c
- **Positive** (P): áº¢nh cÃ¹ng ngÆ°á»i
- **Negative** (N): áº¢nh ngÆ°á»i khÃ¡c

**Loss Function**:

```
L = max(0, ||f(A) - f(P)||Â² - ||f(A) - f(N)||Â² + Î±)

Trong Ä‘Ã³:
- f(x): Embedding function
- ||Â·||: L2 norm
- Î±: Margin (thÆ°á»ng = 0.2)
```

**Má»¥c tiÃªu**:

```
||f(A) - f(P)||Â² + Î± < ||f(A) - f(N)||Â²

NghÄ©a lÃ :
Distance(A, P) + margin < Distance(A, N)
```

### 2.5 Hard Negative Mining

**Váº¥n Ä‘á»**: Háº§u háº¿t triplets Ä‘á»u "dá»…" (loss = 0)

**Giáº£i phÃ¡p**: Chá»‰ chá»n hard triplets:

1. **Hard Positive**:

   ```
   argmax ||f(A) - f(P)||Â²
   Pâˆˆ{same_identity}
   ```

2. **Hard Negative**:

   ```
   argmin ||f(A) - f(N)||Â²
   Nâˆˆ{diff_identity}
   ```

3. **Semi-Hard Negative**:
   ```
   ||f(A) - f(P)||Â² < ||f(A) - f(N)||Â² < ||f(A) - f(P)||Â² + Î±
   ```

### 2.6 Face Verification

**Input**: 2 face images
**Output**: Same person (Yes/No)

**Algorithm**:

```python
def verify(face1, face2, threshold=0.6):
    # Extract embeddings
    emb1 = facenet(face1)  # 512-D vector
    emb2 = facenet(face2)  # 512-D vector

    # Calculate distance
    distance = np.linalg.norm(emb1 - emb2)

    # Decision
    if distance < threshold:
        return "Same person"
    else:
        return "Different people"
```

**Euclidean Distance**:

```
d = âˆš(Î£áµ¢(emb1áµ¢ - emb2áµ¢)Â²)
```

**Cosine Similarity** (alternative):

```
similarity = (emb1 Â· emb2) / (||emb1|| Ã— ||emb2||)

if similarity > threshold:
    â†’ Same person
```

### 2.7 Face Recognition (1:N)

**Input**: 1 face image, N registered faces
**Output**: Identity (or Unknown)

**Algorithm**:

```python
def recognize(face, database, threshold=0.6):
    # Extract embedding
    emb_query = facenet(face)

    # Compare with all in database
    min_distance = float('inf')
    best_match = None

    for identity, emb_db in database.items():
        distance = np.linalg.norm(emb_query - emb_db)

        if distance < min_distance:
            min_distance = distance
            best_match = identity

    # Decision
    if min_distance < threshold:
        return best_match
    else:
        return "Unknown"
```

---

## 3. Anti-Spoofing Methods

### 3.1 Tá»•ng quan

**Má»¥c tiÃªu**: PhÃ¢n biá»‡t khuÃ´n máº·t tháº­t (live) vs giáº£ (spoof)

**Loáº¡i táº¥n cÃ´ng**:

1. Photo attack (áº£nh in)
2. Video replay attack
3. 3D mask attack

### 3.2 Method 1: Texture Analysis

#### 3.2.1 Local Binary Pattern (LBP)

**NguyÃªn lÃ½**: áº¢nh in cÃ³ texture Ä‘á»“ng Ä‘á»u hÆ¡n da ngÆ°á»i tháº­t

**Algorithm**:

```python
def compute_lbp(image, point=(x, y)):
    center = image[y, x]
    code = 0

    # 8 neighbors
    neighbors = [
        image[y-1, x-1], image[y-1, x], image[y-1, x+1],
        image[y, x+1], image[y+1, x+1], image[y+1, x],
        image[y+1, x-1], image[y, x-1]
    ]

    for i, neighbor in enumerate(neighbors):
        if neighbor > center:
            code |= (1 << i)

    return code
```

**LBP Code** (8-bit):

```
       nâ‚€  nâ‚  nâ‚‚
       nâ‚‡  c   nâ‚ƒ
       nâ‚†  nâ‚…  nâ‚„

LBP = Î£áµ¢ s(náµ¢ - c) Ã— 2â±

Trong Ä‘Ã³:
s(x) = 1 if x â‰¥ 0
       0 if x < 0
```

**Features**:

- Histogram of LBP codes
- Variance of LBP
- Entropy of LBP distribution

#### 3.2.2 Frequency Domain Analysis (FFT)

**NguyÃªn lÃ½**: áº¢nh in thiáº¿u high-frequency components

**Algorithm**:

```python
def frequency_analysis(image):
    # Convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

    # Apply FFT
    fft = np.fft.fft2(gray)
    fft_shift = np.fft.fftshift(fft)

    # Magnitude spectrum
    magnitude = np.abs(fft_shift)

    # Analyze frequency distribution
    H, W = magnitude.shape
    center_h, center_w = H//2, W//2

    # High frequency ratio
    high_freq = (magnitude[0:center_h//2, :].sum() +
                 magnitude[center_h+center_h//2:, :].sum())
    total_freq = magnitude.sum()

    ratio = high_freq / total_freq

    # Real face: ratio > threshold
    return ratio
```

**Power Spectrum**:

```
P(u, v) = |F(u, v)|Â²

Trong Ä‘Ã³:
- F(u, v): Fourier transform
- (u, v): Frequency coordinates
```

### 3.3 Method 2: Blink Detection

#### 3.3.1 Eye Aspect Ratio (EAR)

**NguyÃªn lÃ½**: áº¢nh in khÃ´ng nhÃ¡y máº¯t

**Formula**:

```
       pâ‚      pâ‚‚
         â•±â”€â”€â•²
        â”‚    â”‚
         â•²__â•±
       pâ‚†      pâ‚…
     pâ‚ƒ        pâ‚„

EAR = (||pâ‚‚ - pâ‚†|| + ||pâ‚ƒ - pâ‚…||) / (2 Ã— ||pâ‚ - pâ‚„||)

Trong Ä‘Ã³:
- pâ‚, pâ‚„: Horizontal eye corners
- pâ‚‚, pâ‚ƒ, pâ‚…, pâ‚†: Vertical eye points
```

**Properties**:

- Open eye: EAR â‰ˆ 0.3
- Closed eye: EAR â‰ˆ 0.1
- Blink: EAR drops then rises

**Blink Detection Algorithm**:

```python
def detect_blink(ear_sequence, threshold=0.2, consec_frames=3):
    blink_counter = 0
    blink_total = 0

    for ear in ear_sequence:
        if ear < threshold:
            blink_counter += 1
        else:
            if blink_counter >= consec_frames:
                blink_total += 1
            blink_counter = 0

    return blink_total
```

### 3.4 Method 3: Motion Analysis

#### 3.4.1 Optical Flow

**NguyÃªn lÃ½**: Video replay cÃ³ motion pattern khÃ¡c ngÆ°á»i tháº­t

**Lucas-Kanade Method**:

```
Assumptions:
1. Brightness constancy: I(x,y,t) = I(x+dx,y+dy,t+dt)
2. Small motion: Taylor expansion

I(x+dx,y+dy,t+dt) â‰ˆ I(x,y,t) + âˆ‚I/âˆ‚xÂ·dx + âˆ‚I/âˆ‚yÂ·dy + âˆ‚I/âˆ‚tÂ·dt

Optical Flow Equation:
Iâ‚“Â·u + Iáµ§Â·v + Iâ‚œ = 0

Trong Ä‘Ã³:
- (u, v): Velocity vector
- Iâ‚“, Iáµ§: Spatial gradients
- Iâ‚œ: Temporal gradient
```

**Farneback Method** (denser flow):

```python
flow = cv2.calcOpticalFlowFarneback(
    prev_gray, next_gray, None,
    pyr_scale=0.5,    # Pyramid scale
    levels=3,         # Number of pyramid layers
    winsize=15,       # Averaging window size
    iterations=3,     # Iterations at each level
    poly_n=5,         # Polynomial expansion
    poly_sigma=1.2,   # Gaussian std
    flags=0
)

# Magnitude and angle
magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])
```

**Features**:

1. **Magnitude Variance**:

   ```
   ÏƒÂ²(mag) = E[(mag - Î¼)Â²]
   ```

2. **Direction Entropy**:

   ```
   H(angle) = -Î£áµ¢ p(Î¸áµ¢)Â·log(p(Î¸áµ¢))
   ```

3. **Average Magnitude**:
   ```
   Î¼(mag) = (1/N)Â·Î£áµ¢ mag(i)
   ```

### 3.5 Method 4: Depth Analysis

#### 3.5.1 Gradient-based Depth

**NguyÃªn lÃ½**: KhuÃ´n máº·t 3D cÃ³ gradient phá»©c táº¡p hÆ¡n áº£nh 2D

**Sobel Operator**:

```
Gâ‚“ = [-1  0  1]       Gáµ§ = [-1 -2 -1]
     [-2  0  2]            [ 0  0  0]
     [-1  0  1]            [ 1  2  1]

Gradient magnitude:
G = âˆš(Gâ‚“Â² + Gáµ§Â²)

Gradient direction:
Î¸ = atan2(Gáµ§, Gâ‚“)
```

**Features**:

```python
def depth_features(image):
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

    # Sobel gradients
    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)
    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)

    # Magnitude
    magnitude = np.sqrt(sobelx**2 + sobely**2)

    # Features
    features = {
        'mean_gradient': np.mean(magnitude),
        'std_gradient': np.std(magnitude),
        'max_gradient': np.max(magnitude)
    }

    return features
```

#### 3.5.2 Shadow/Highlight Analysis

**NguyÃªn lÃ½**: KhuÃ´n máº·t 3D cÃ³ shadow/highlight tá»± nhiÃªn

**Otsu's Thresholding**:

```
Maximize inter-class variance:
ÏƒÂ²_between = wâ‚€(Î¼â‚€ - Î¼_total)Â² + wâ‚(Î¼â‚ - Î¼_total)Â²

Trong Ä‘Ã³:
- wâ‚€, wâ‚: Class weights
- Î¼â‚€, Î¼â‚: Class means
- Î¼_total: Total mean
```

**Contrast Ratio**:

```
CR = (L_max - L_min) / (L_max + L_min)

Trong Ä‘Ã³:
- L_max: Brightest region
- L_min: Darkest region
```

### 3.6 Liveness Score Fusion

**Multi-method Fusion**:

```
L = wâ‚Â·S_texture + wâ‚‚Â·S_blink + wâ‚ƒÂ·S_motion + wâ‚„Â·S_depth

Trong Ä‘Ã³:
- Sáµ¢ âˆˆ [0, 1]: Normalized score cá»§a method i
- wáµ¢: Weight (Î£wáµ¢ = 1)
- L â‰¥ threshold â†’ LIVE
- L < threshold â†’ SPOOF
```

**Weighted Average** (default):

```
wâ‚ = wâ‚‚ = wâ‚ƒ = wâ‚„ = 0.25
```

**Adaptive Weighting** (advanced):

```
wáµ¢ = conf(Sáµ¢) / Î£â±¼ conf(Sâ±¼)

Trong Ä‘Ã³:
conf(S): Confidence of score S
```

---

## 4. Pipeline Tá»•ng Thá»ƒ

### 4.1 Attendance System Flowchart

```
START
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Capture Frame    â”‚
â”‚ from Camera      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MTCNN            â”‚ â† Face Detection
â”‚ Detect Faces     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Has Face?
    â†™     â†˜
  Yes      No â†’ Return to Capture
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Anti-Spoofing    â”‚ â† Liveness Check
â”‚ Analysis         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Is Real?
    â†™     â†˜
  Yes      No â†’ Reject + Log Spoofing
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FaceNet          â”‚ â† Extract Embedding
â”‚ Get Embedding    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Compare with     â”‚ â† Face Recognition
â”‚ Database         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
   Match Found?
    â†™     â†˜
  Yes      No â†’ Display "Unknown"
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Check Time       â”‚ â† Attendance Rule
â”‚ Constraints      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Valid?
    â†™     â†˜
  Yes      No â†’ Skip (Too soon)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Log Attendance   â”‚ â† Database Write
â”‚ to Database      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Update UI        â”‚ â† Display Result
â”‚ Show Success     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
        END
```

### 4.2 Registration Flow

```
START
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input Employee   â”‚
â”‚ Information      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Capture N images â”‚ (N = 5 default)
â”‚ from Camera      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For each image:  â”‚
â”‚ - MTCNN detect   â”‚
â”‚ - Quality check  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FaceNet extract  â”‚
â”‚ N embeddings     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Average          â”‚
â”‚ embeddings       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save to:         â”‚
â”‚ - SQLite DB      â”‚
â”‚ - Embeddings PKL â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
        END
```

---

## 5. ToÃ¡n Há»c Äáº±ng Sau

### 5.1 Convolutional Neural Networks

**Convolution Operation**:

```
(I * K)(x, y) = Î£Î£ I(x+i, y+j)Â·K(i, j)
                i j

Trong Ä‘Ã³:
- I: Input image
- K: Kernel/filter
- *: Convolution operator
```

**Pooling**:

```
Max Pooling:
P(x, y) = max{I(x+i, y+j) | i,j âˆˆ window}

Average Pooling:
P(x, y) = (1/|W|)Â·Î£I(x+i, y+j)
```

### 5.2 Activation Functions

**ReLU**:

```
f(x) = max(0, x)
f'(x) = {1 if x > 0, 0 if x â‰¤ 0}
```

**PReLU** (Parametric ReLU):

```
f(x) = {x if x > 0, Î±Â·x if x â‰¤ 0}

Trong Ä‘Ã³ Î± Ä‘Æ°á»£c há»c trong training
```

### 5.3 Batch Normalization

```
Î¼_B = (1/m)Â·Î£xáµ¢          # Mean
ÏƒÂ²_B = (1/m)Â·Î£(xáµ¢ - Î¼_B)Â² # Variance

xÌ‚áµ¢ = (xáµ¢ - Î¼_B) / âˆš(ÏƒÂ²_B + Îµ)  # Normalize

yáµ¢ = Î³Â·xÌ‚áµ¢ + Î²            # Scale and shift

Trong Ä‘Ã³:
- m: Batch size
- Îµ: Small constant (10â»âµ)
- Î³, Î²: Learnable parameters
```

### 5.4 Loss Functions

**Cross-Entropy Loss**:

```
L = -Î£áµ¢ yáµ¢Â·log(Å·áµ¢)

Trong Ä‘Ã³:
- yáµ¢: True label (one-hot)
- Å·áµ¢: Predicted probability
```

**Smooth L1 Loss**:

```
Lâ‚_smooth(x) = {0.5Â·xÂ²      if |x| < 1
               {|x| - 0.5   otherwise
```

**Triplet Loss**:

```
L = Î£[||f(A) - f(P)||Â² - ||f(A) - f(N)||Â² + Î±]â‚Š

Trong Ä‘Ã³ [x]â‚Š = max(0, x)
```

### 5.5 Optimization

**Stochastic Gradient Descent**:

```
Î¸â‚œâ‚Šâ‚ = Î¸â‚œ - Î·Â·âˆ‡L(Î¸â‚œ)

Trong Ä‘Ã³:
- Î¸: Parameters
- Î·: Learning rate
- âˆ‡L: Gradient of loss
```

**Adam Optimizer**:

```
mâ‚œ = Î²â‚Â·mâ‚œâ‚‹â‚ + (1-Î²â‚)Â·gâ‚œ      # First moment
vâ‚œ = Î²â‚‚Â·vâ‚œâ‚‹â‚ + (1-Î²â‚‚)Â·gâ‚œÂ²     # Second moment

mÌ‚â‚œ = mâ‚œ/(1-Î²â‚áµ—)               # Bias correction
vÌ‚â‚œ = vâ‚œ/(1-Î²â‚‚áµ—)

Î¸â‚œ = Î¸â‚œâ‚‹â‚ - Î·Â·mÌ‚â‚œ/âˆš(vÌ‚â‚œ + Îµ)

Trong Ä‘Ã³:
- Î²â‚ = 0.9, Î²â‚‚ = 0.999
- Îµ = 10â»â¸
```

### 5.6 Metrics

**Euclidean Distance**:

```
d(x, y) = âˆš(Î£áµ¢(xáµ¢ - yáµ¢)Â²) = ||x - y||â‚‚
```

**Cosine Distance**:

```
d_cos(x, y) = 1 - (xÂ·y)/(||x||Â·||y||)
```

**Accuracy**:

```
Acc = (TP + TN) / (TP + TN + FP + FN)
```

**F1 Score**:

```
Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
F1 = 2Â·(PrecisionÂ·Recall)/(Precision + Recall)
```

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

### Papers:

1. **MTCNN**:

   - Zhang, K., et al. (2016). "Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks." IEEE Signal Processing Letters.

2. **FaceNet**:

   - Schroff, F., et al. (2015). "FaceNet: A Unified Embedding for Face Recognition and Clustering." CVPR.

3. **InceptionNet**:

   - Szegedy, C., et al. (2017). "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning." AAAI.

4. **Anti-Spoofing**:

   - Boulkenafet, Z., et al. (2016). "Face Spoofing Detection Using Colour Texture Analysis." IEEE TIFS.
   - Chingovska, I., et al. (2012). "On the Effectiveness of Local Binary Patterns in Face Anti-spoofing." BIOSIG.

5. **Triplet Loss**:
   - Hermans, A., et al. (2017). "In Defense of the Triplet Loss for Person Re-Identification." arXiv.

---

## ğŸ“ Káº¾T LUáº¬N

Document nÃ y cung cáº¥p giáº£i thÃ­ch chi tiáº¿t vá»:

- âœ… Kiáº¿n trÃºc MTCNN (3 stages)
- âœ… FaceNet vá»›i Triplet Loss
- âœ… 4 phÆ°Æ¡ng phÃ¡p Anti-Spoofing
- âœ… Pipeline xá»­ lÃ½
- âœ… CÃ´ng thá»©c toÃ¡n há»c

CÃ³ thá»ƒ sá»­ dá»¥ng trá»±c tiáº¿p cho:

- ğŸ“ BÃ¡o cÃ¡o tiá»ƒu luáº­n
- ğŸ“Š Slide thuyáº¿t trÃ¬nh
- ğŸ“– Paper náº¿u publish

---

**Made for academic purposes** ğŸ“
**Date**: October 2025
